# chat-llama-cpp-sample
A simple web app as example of use server API of [llama.cpp](https://github.com/ggerganov/llama.cpp).

Support prompt generation for the following models:
* [Vicuna v1](https://huggingface.co/chharlesonfire/ggml-vicuna-7b-f16)

## Run the project

You need NodeJS to run this project.

```bash
git clone https://github.com/FSSRepo/chat-llama-cpp-sample.git
cd chat-llama-cpp-sample
npm install
node .
```

The web app listen in `localhost:2400`.

## Screenshots

The model on the screenshots is pygmalion.

*Screenshot 1*

![Screenshot](images/sample-1.png)

*Screenshot 2*

![Screenshot](images/sample-2.png)